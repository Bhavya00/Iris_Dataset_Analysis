## Data Preprocessing 
# Importing Dataset

dataset=read.csv("iris.csv")
# Splitting dataset into testset and training set
#install.packages("caTools")
# install.packages("MASS")
#install.packages("ggplot.multistats")
#install.packages("ggplot2")

library(ggplot2) # Data visualization
## Data Visualisation 

# boxplot
irisVer= subset(dataset, species == "versicolor")
irisSet=subset(dataset, species == "setosa")
irisVir=subset(dataset, species == "virginica")
par(mfrow=c(1,3),mar=c(6,3,2,1))
boxplot(irisVer[,1:4], main="Versicolor",ylim = c(0,8),las=2, col="bisque")
boxplot(irisSet[,1:4], main="Setosa",ylim = c(0,8),las=2,col="cadetblue1")
boxplot(irisVir[,1:4], main="Virginica",ylim = c(0,8),las=2,col="coral1")

# histogram 
par(mfrow=c(1,3))
hist(irisVer$petal_length,breaks=seq(0,8,l=17),xlim=c(0,8),ylim=c(0,40),col ="antiquewhite" )
hist(irisSet$petal_length,breaks=seq(0,8,l=17),xlim=c(0,8),ylim=c(0,40),col = "chartreuse")
hist(irisVir$petal_length,breaks=seq(0,8,l=17),xlim=c(0,8),ylim=c(0,40),col = "deepskyblue1")

#Pie Chart
table(dataset$species)
pie(table(dataset$species),main = "Pie chart species",col=c("red","blue","coral"),radius = 1)
# Scatter Plot 
#g=ggplot(dataset,aes(x=sepal_length,y=sepal_width))
#g=g + geom_point(aes(shape = factor(species), colour = factor(species)))
#g=g + ggtitle (" SepalLength Vs SepalWidth wrt Species" )
#g=g + stat_smooth(method= lm)
#g

#Identifying correlation
pairs(dataset[,1:4],col=dataset[,5],oma=c(4,4,6,12))
par(xpd=TRUE)
legend(0.85,0.6, as.vector(unique(dataset$species)),fill=c(1,2,3))

# Using 3D Plot 
#install.packages("plot3D")
# install.packages(c("rgl", "car"))
install.packages("car")
library("car")
scatter3d(x = dataset$sepal_length, y = dataset$sepal_width, z = dataset$petal_length)

# Encoding categorical data by using factor function

dataset$species=factor(dataset$species,levels=c("setosa","versicolor","virginica"),labels=c(1,2,3))

# DESICION TREE CLASSIFICATION 
#install.packages("C50")
library(C50)
input=dataset[,1:4]
output= dataset[,5]
model1=C5.0(input, output, control = C5.0Control(noGlobalPruning = TRUE,minCases=1))
plot(model1, main="C5.0 Decision Tree - Unpruned, min=1")

model2 <- C5.0(input, output, control = C5.0Control(noGlobalPruning = FALSE))
plot(model2, main="C5.0 Decision Tree - Pruned")
summary(model2)

# RANDOM FOREST CLASSIFICATION
#install.packages("randomForest") 
library(randomForest) 
iris.rf=randomForest(species ~ .,  
                        data = dataset,  
                        importance = TRUE, 
                        proximity = TRUE) 
print(iris.rf) 
png(file = "randomForestClassification.png") 

# Plot the error vs the number of trees graph 
plot(iris.rf) 

# Saving the file 
 dev.off() 


#library(caTools)
#set.seed(123)   #  set seed to ensure we always have same random numbers generated
#sample= sample.split(dataset$species,SplitRatio = 0.8) # splits the data in the ratio mentioned in SplitRatio. After splitting marks these rows as logical TRUE and the the remaining are marked as logical FALSE
#training_set =subset(dataset,sample ==TRUE) # creates a training dataset named train1 with rows which are marked as TRUE
#test_set=subset(dataset, sample ==FALSE)

# Feature Scaling  
#training_set[,2:3] = scale(training_set[,2:3])
#test_set[,2:3]= scale(test_set[,2:3])



